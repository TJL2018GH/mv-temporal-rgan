** Workflow changes

*** Model stabilization and abstraction
***** TODO adapt workflow from existing successful GAN architecture and modify for our task
***** TODO consider adding Gaussian noise to images for stability (mixed outcomes predicted)
***** TODO spectral normalization for convolutional layers without batch normalization (check if works), add citation for it in due time
***** consider using attention layer for encoder-decoder archtecture 
***** consider using stacked LSTMs which only look at some timesteps at a time, since complete evaluation leads to seeming independence in time
***** consider having online similarity checks, MMD and TRTS to check quality of samples
***** use Wasserstein loss with standard or improved training
***** consider resnet architecture for certain skip-connections, could be linked to multi-scale gradient structure
***** use feature matching and minibatch discrimination to prevent mode collapse
***** look into ConvSN1D rate rank 1 error, thoroughly review eth rgan model for comparison 
***** consider averaging with various dilations for discriminator phase, could be linked to resnet
***** work on more efficient (automated) hard model memory handling (saving only one instance of weights in comb.h5 and abstracting via layer numbers) -> necessary for github push
***** export optimizer weights as h5 instead of pickle for data consistency and compactness
***** make pipeline variable/adaptable/scalable to higher (possibly non-square) dimensional data in case of 64 dimensional lfw faces (user more variables in models instead of hard-coding)
***** extend models to RCGANs once results are satisfactory

*** Model visualization and presentation
***** TODO update RGAN version 2/3 with performance summary and manage branches
***** work on better connection between readme and development-log by piping certain points on pre-commit hooks; or otherwise manage caveats in readme actively
***** make todos.org look better on github with proper dates and formatting
***** make waveform type of visualization of datasets and generated products for preliminary datasets and MIMIC-III
***** add function to generate best samples from trained model aside from already generated image
***** change matplotlib backend default back to instant working version when necessary
***** add MIMIC-III 2d projection depiction and learning as gif on initial readme
***** remove caveats in readme once relevant developments are complete

*** Model application to biomedical time series
***** visualize data from MIMIC-III github repository in 2-dimensions to see smoothness or roughness
***** apply RCGAN technique towards this process and verify results with existing models through TSTR/TRTS and MMD checks

*** Feedback from discussions
***** pooling/attention LSTMs for higher look-back capability
***** extra time distribution between rows/sequences
***** layer vs. batch normalization
***** attempt SELU layer if easily available
***** more stable implementations of generative models such as R-autoencoders
***** architectures which do not suffer from mode collapse, eg. autoencoder/non-variational/transformer

*** GPU management
***** try to share memory and tasks between gpus (multi-gpu-model)
***** try to use memory limiting in code in case GPUs are already being used

*** Heuristics
***** add gradient checks for logging and change vis.py to include colour on loss line for gradients
***** set up crowd-sourced grid-search via emails to check results
***** optionally use tensorboard to analyze learning process
***** look for similarity measure metrics which could be used alongside training

*** Backup Google Colab
***** work on google drive cli access for streamlined model runs
***** link google colab to local jupyter runtime
***** run multiple notebooks directly from computer without browser
***** sync google drive to localhost for easy access

*** Clean-code/documentation
***** track how many epochs or batch runs needed to converge and try to optimize this (~1000 for good results)
***** add conditions to "train.py" to add separate pipeline in RCGAN training

*** Additional improvements
***** look into unsupervised feature extraction in ML
***** isolate personal identification features in discriminator from generated time series
***** use adversarial samples to generate bad data that network falsely predicts

*** Brainstorming points
**** GAN stabilisation:
***** Gaussian label smoothing
***** differing learning rates for optimizers
***** Gaussian noise addition to images
***** spectral normalization
***** multi-scale gradient
**** Evaluation pipeline
***** use MIMIC data/models for direct MMD + TSTR/TRTS validations
***** explore privacy perspective and whether GAN is able to remove personal traits
***** or consider another architecture which can perform this function
**** Networks and higher-dimensions abstraction
***** extend to deeper model which can handle 64 pixels faces to check if abstraction possible
***** extend to RCGAN with realistic conditionings for actual usable data genration
**** Input images and feature masking
***** come up with mask to simulate missing data in real-life
***** compare input and output images as time series with signals
**** Documentation and code-health:
***** fix unused imports and sort with python tools
***** make proper documentation and model visualizations
