** Workflow changes

*** Model visualization and presentation
***** TODO add initial model summary/architectures/feedback/performance into readme
***** work on better connection between readme and development-log by piping certain points on pre-commit hooks; or otherwise manage caveats in readme actively
***** make todos.org look better on github with proper dates and formatting
***** make waveform type of visualization of datasets and generated products for preliminary datasets and MIMIC-III
***** add function to generate best samples from trained model aside from already generated image
***** change matplotlib backend default back to instant working version when necessary
***** add MIMIC-III 2d projection depiction and learning as gif on initial readmue
***** remove caveats in readme once relevant developments are complete

*** Model stabilization and abstraction
***** TODO use Gaussian noise addition to images (to prevent mode collapse and add robustness) and Gaussian-noise label smoothing (non-model)
***** TODO use multi-scale gradient and spectral normalization (model), first non-model then model-based changes 
***** TODO make network deeper and review eth rgan model for comparison
***** TODO use Wasserstein loss and dilations for larger scale temporal relations
***** TODO consider removing LSTM in generator and adding additional LSTM in discriminator
***** TODO use rows as channels/time-steps, separate rows from temporal pipeline for better convergence
***** TODO work on more efficient hard model memory handling (saving only one instance of weights in comb.h5 and abstracting via layer numbers)
***** after making above mode-based changes, run all 3 data-based models to see results
***** extend models to RCGANs once results are satisfactory
***** make pipeline variable/adaptable/scalable to higher dimensional data in case of 64 dimensional lfw faces

*** Model application to biomedical time series
***** visualize data from MIMIC-III github repository in 2-dimensions to see smoothness or roughness
***** apply RCGAN technique towards this process and verify results with existing models through TSTR/TRTS and MMD checks

*** Feedback from discussions
***** pooling/attention LSTMs for higher look-back capability
***** extra time distribution between rows/sequences
***** layer vs. batch normalization
***** attempt SELU layer if easily available
***** more stable implementations of generative models such as R-autoencoders
***** architectures which do not suffer from mode collapse, eg. autoencoder/non-variational/transformer

*** GPU management
***** try to share memory and tasks between gpus (multi_gpu_model)
***** try to use memory limiting in code in case GPUs are already being used

*** Heuristics
***** add gradient checks for logging and change vis.py to include colour on loss line for gradients
***** set up crowd-sourced grid-search via emails to check results
***** optionally use tensorboard to analyze learning process
***** look for similarity measure metrics which could be used alongside training

*** Backup Google Colab
***** work on google drive cli access for streamlined model runs
***** link google colab to local jupyter runtime
***** run multiple notebooks directly from computer without browser
***** sync google drive to localhost for easy access

*** Clean-code/documentation
***** track how many epochs or batch runs needed to converge and try to optimize this (~1000 for good results)
***** add conditions to "train.py" to add separate pipeline in RCGAN training

*** Additional improvements
***** look into unsupervised feature extraction in ML
***** isolate personal identification features in discriminator from generated time series
***** use adversarial samples to generate bad data that network falsely predicts

*** Brainstorming points
**** GAN stabilisation:
***** Gaussian label smoothing
***** differing learning rates for optimizers
***** Gaussian noise addition to images
***** spectral normalization
***** multi-scale gradient
**** Evaluation pipeline
***** use MIMIC data/models for direct MMD + TSTR/TRTS validations
***** explore privacy perspective and whether GAN is able to remove personal traits
***** or consider another architecture which can perform this function
**** Networks and higher-dimensions abstraction
***** extend to deeper model which can handle 64 pixels faces to check if abstraction possible
***** extend to RCGAN with realistic conditionings for actual usable data genration
**** Input images and feature masking
***** come up with mask to simulate missing data in real-life
***** compare input and output images as time series with signals
**** Documentation and code-health:
***** fix unused imports and sort with python tools
***** make proper documentation and model visualizations
